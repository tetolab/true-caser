{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten\n",
    "from keras.layers import TimeDistributed, Bidirectional, InputLayer, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.metrics import categorical_accuracy\n",
    "from IPython.display import clear_output\n",
    "from more_itertools import flatten, intersperse\n",
    "import random\n",
    "from batcher import batch_from_generator\n",
    "from train_data import load_conll2003, create_conll_encoded_shifted_generator\n",
    "from mappings import get_all_mappings\n",
    "from gutenberg import gutenberg_training_data_generator, gutenberg_book_generator_from_website\n",
    "from padder import pad\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.2\n",
    "TIME_SLICE_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "SAMPLING_RATE = 1\n",
    "PADDING = 0\n",
    "UNKNOWN = 1\n",
    "NUM_OF_UNITS = TIME_SLICE_SIZE * 4\n",
    "WORDS_PER_BATCH = 100\n",
    "EPOCHS=1\n",
    "MODEL_SAVE_PATH = 'tc_model.h5'\n",
    "LSTM_MODEL_SAVE_PATH = 'lstm_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, lower_mapping, lower_reverse_mapping = get_all_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(shape, classes):\n",
    "    # First layer inputs must be 3D\n",
    "    # with shape (samples, timesteps, features)\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=shape))\n",
    "    model.add(Bidirectional(LSTM(NUM_OF_UNITS, return_sequences=True, dropout=DROPOUT, recurrent_dropout=DROPOUT)))\n",
    "    model.add(Bidirectional(LSTM(NUM_OF_UNITS, return_sequences=True, dropout=DROPOUT, recurrent_dropout=DROPOUT)))\n",
    "    model.add(TimeDistributed(Dense(classes)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model((TIME_SLICE_SIZE, len(mapping)), len(mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(MODEL_SAVE_PATH): \n",
    "    model.load_weights(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(batch_size=BATCH_SIZE, write_graph=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printed_times = 0\n",
    "g = gutenberg_training_data_generator(TIME_SLICE_SIZE, BATCH_SIZE, shift=False)\n",
    "print('training')\n",
    "for i in range(0, 1000):\n",
    "    X = None\n",
    "    Y = None\n",
    "    try:\n",
    "        X, Y = next(g)\n",
    "    except StopIteration:\n",
    "        g = gutenberg_training_data_generator(TIME_SLICE_SIZE, BATCH_SIZE, shift=False)\n",
    "        X, Y = next(g)\n",
    "    if printed_times > 10:\n",
    "        clear_output()\n",
    "        print(f'epoch: {i}')\n",
    "        printed_times = 0\n",
    "\n",
    "    model.fit(X, Y,  verbose=2, validation_split=0.2, callbacks=[tensor_board])\n",
    "    model.save(LSTM_MODEL_SAVE_PATH)\n",
    "    printed_times += 1\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = pad(\"Tim Hawkins born in 1989 in Birkenhead\".lower(), TIME_SLICE_SIZE)\n",
    "mapped_sentence = [lower_mapping[char] for char in test_sentence]\n",
    "mapped_sentence = pad_sequences([mapped_sentence], maxlen=TIME_SLICE_SIZE, padding='post')\n",
    "mapped_sentence = to_categorical(mapped_sentence, len(mapping))\n",
    "\n",
    "mapped_sentence = np.asarray(mapped_sentence)\n",
    "#mapped_sentence = np.reshape(1, len(test_sentence))\n",
    "predicted_result = model.predict_classes(mapped_sentence)\n",
    "predicted_result = [reverse_mapping[i] for i in predicted_result[0]]\n",
    "''.join(predicted_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
