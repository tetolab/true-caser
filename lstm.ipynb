{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten\n",
    "from keras.layers import TimeDistributed, Bidirectional, InputLayer, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.metrics import categorical_accuracy\n",
    "from IPython.display import clear_output\n",
    "from more_itertools import flatten, intersperse\n",
    "import random\n",
    "\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from batcher import batch_from_generator\n",
    "from train_data import load_conll2003, create_conll_encoded_shifted_generator\n",
    "from mappings import get_all_mappings, gen_input_feature_to_class_map, gen_input_feature_to_int_map\n",
    "from corpus import corpus_training_data_generator, create_all_corpus_train_pipeline, pad, encode_each_sentence\n",
    "from model import create_model, compile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.2\n",
    "TIME_SLICE_SIZE = 50\n",
    "BATCH_SIZE = 512\n",
    "SAMPLING_RATE = 1\n",
    "OUTPUT_CLASSES = 4\n",
    "PADDING = 0\n",
    "UNKNOWN = 1\n",
    "NUM_OF_UNITS = 100\n",
    "WORDS_PER_BATCH = 100\n",
    "EPOCHS=1\n",
    "MODEL_SAVE_PATH = 'tc_model.h5'\n",
    "LSTM_MODEL_SAVE_PATH = 'lstm_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, lower_mapping, lower_reverse_mapping = get_all_mappings()\n",
    "input_feature_to_class_map = gen_input_feature_to_class_map()\n",
    "input_feature_to_int_map = gen_input_feature_to_int_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(shape, classes):\n",
    "#     # First layer inputs must be 3D\n",
    "#     # with shape (samples, timesteps, features)\n",
    "#     model = Sequential()\n",
    "#     model.add(InputLayer(input_shape=shape))\n",
    "#     model.add(Bidirectional(LSTM(NUM_OF_UNITS, return_sequences=True, dropout=DROPOUT, recurrent_dropout=DROPOUT)))\n",
    "#     model.add(TimeDistributed(Dense(1000)))\n",
    "#     model.add(TimeDistributed(Dense(classes)))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(1, 4, NUM_OF_UNITS, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(MODEL_SAVE_PATH): \n",
    "    model.load_weights(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(batch_size=BATCH_SIZE, write_graph=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, Y = create_all_corpus_train_pipeline(TIME_SLICE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 273659 samples, validate on 68415 samples\n",
      "Epoch 1/100\n",
      "273659/273659 [==============================] - 137s 500us/step - loss: 0.0681 - acc: 0.9800 - val_loss: 0.0495 - val_acc: 0.9876\n",
      "Epoch 2/100\n",
      "273659/273659 [==============================] - 137s 500us/step - loss: 0.0668 - acc: 0.9803 - val_loss: 0.0474 - val_acc: 0.9882\n",
      "Epoch 3/100\n",
      "273659/273659 [==============================] - 137s 502us/step - loss: 0.0662 - acc: 0.9804 - val_loss: 0.0461 - val_acc: 0.9885\n",
      "Epoch 4/100\n",
      "273659/273659 [==============================] - 138s 503us/step - loss: 0.0658 - acc: 0.9805 - val_loss: 0.0461 - val_acc: 0.9887\n",
      "Epoch 5/100\n",
      "273659/273659 [==============================] - 139s 507us/step - loss: 0.0654 - acc: 0.9805 - val_loss: 0.0477 - val_acc: 0.9883\n",
      "Epoch 6/100\n",
      "273659/273659 [==============================] - 135s 494us/step - loss: 0.0650 - acc: 0.9806 - val_loss: 0.0479 - val_acc: 0.9880\n",
      "Epoch 7/100\n",
      "273659/273659 [==============================] - 134s 491us/step - loss: 0.0647 - acc: 0.9807 - val_loss: 0.0494 - val_acc: 0.9874\n",
      "Epoch 8/100\n",
      "273659/273659 [==============================] - 134s 491us/step - loss: 0.0641 - acc: 0.9808 - val_loss: 0.0477 - val_acc: 0.9879\n",
      "Epoch 9/100\n",
      "106496/273659 [==========>...................] - ETA: 1:18 - loss: 0.0641 - acc: 0.9807"
     ]
    }
   ],
   "source": [
    "\n",
    "#g = corpus_training_data_generator('gutenberg',TIME_SLICE_SIZE, BATCH_SIZE, shift=False)\n",
    "model.fit(X, Y, validation_split=0.2, callbacks=[tensor_board], batch_size=512, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'his name was michael smith, he lived in newcastle.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentence = \"his name was Michael Smith, he lived in Newcastle.\".lower()\n",
    "test_sentence = pad([original_sentence], TIME_SLICE_SIZE)\n",
    "test_sentence = encode_each_sentence(test_sentence, input_feature_to_int_map)\n",
    "\n",
    "mapped_sentence = np.asarray(test_sentence)\n",
    "predicted_result = model.predict_classes(mapped_sentence)[0]\n",
    "print(predicted_result)\n",
    "predicted_result = list(zip(pad([original_sentence], TIME_SLICE_SIZE)[0], predicted_result.tolist()))\n",
    "\n",
    "def true_case(letter, label):\n",
    "    if letter == 0:\n",
    "        return ''\n",
    "    if(label == 2):\n",
    "        return letter.lower()\n",
    "    if (label == 3):\n",
    "        return letter.upper()\n",
    "    if (label == 0):\n",
    "        return ''\n",
    "    return letter\n",
    "\n",
    "predicted_result = [true_case(letter, label) for letter, label in predicted_result]\n",
    "''.join(predicted_result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"big_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in X[0]:\n",
    "    print(x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
