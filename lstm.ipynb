{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten\n",
    "from keras.layers import TimeDistributed, Bidirectional, InputLayer, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.metrics import categorical_accuracy\n",
    "from IPython.display import clear_output\n",
    "from more_itertools import flatten, intersperse\n",
    "import random\n",
    "\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Todd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from batcher import batch_from_generator\n",
    "from train_data import load_conll2003, create_conll_encoded_shifted_generator\n",
    "from mappings import get_all_mappings, gen_input_feature_to_class_map, gen_input_feature_to_int_map\n",
    "from corpus import corpus_training_data_generator, create_all_corpus_train_pipeline, pad, encode_each_sentence\n",
    "from model import create_model, compile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "TIME_SLICE_SIZE = 50\n",
    "BATCH_SIZE = 512\n",
    "SAMPLING_RATE = 1\n",
    "OUTPUT_CLASSES = 2\n",
    "PADDING = 0\n",
    "UNKNOWN = 1\n",
    "NUM_OF_UNITS = 200\n",
    "EPOCHS=200\n",
    "MODEL_SAVE_PATH = 'tc_model.h5'\n",
    "LSTM_MODEL_SAVE_PATH = 'lstm_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, reverse_mapping, lower_mapping, lower_reverse_mapping = get_all_mappings()\n",
    "input_feature_to_class_map = gen_input_feature_to_class_map()\n",
    "input_feature_to_int_map = gen_input_feature_to_int_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(shape, classes):\n",
    "#     # First layer inputs must be 3D\n",
    "#     # with shape (samples, timesteps, features)\n",
    "#     model = Sequential()\n",
    "#     model.add(InputLayer(input_shape=shape))\n",
    "#     model.add(Bidirectional(LSTM(NUM_OF_UNITS, return_sequences=True, dropout=DROPOUT, recurrent_dropout=DROPOUT)))\n",
    "#     model.add(TimeDistributed(Dense(1000)))\n",
    "#     model.add(TimeDistributed(Dense(classes)))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(1, 2, NUM_OF_UNITS, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(MODEL_SAVE_PATH): \n",
    "    model.load_weights(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(batch_size=BATCH_SIZE, write_graph=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_all_corpus_train_pipeline(TIME_SLICE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = Y * 10\n",
    "W = W + 1\n",
    "W = W.reshape((-1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115001, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 92000 samples, validate on 23001 samples\n",
      "Epoch 1/200\n",
      "92000/92000 [==============================] - 48s 526us/step - loss: 0.5617 - acc: 0.9193 - val_loss: 0.3531 - val_acc: 0.9179\n",
      "Epoch 2/200\n",
      "92000/92000 [==============================] - 45s 493us/step - loss: 0.3624 - acc: 0.9363 - val_loss: 1.3732 - val_acc: 0.7245\n",
      "Epoch 3/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.3099 - acc: 0.9471 - val_loss: 1.5959 - val_acc: 0.7607\n",
      "Epoch 4/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2850 - acc: 0.9517 - val_loss: 1.9980 - val_acc: 0.7379\n",
      "Epoch 5/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2737 - acc: 0.9543 - val_loss: 2.3716 - val_acc: 0.7147\n",
      "Epoch 6/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2653 - acc: 0.9556 - val_loss: 2.3227 - val_acc: 0.7099\n",
      "Epoch 7/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2610 - acc: 0.9564 - val_loss: 2.8665 - val_acc: 0.6970\n",
      "Epoch 8/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2573 - acc: 0.9572 - val_loss: 3.1863 - val_acc: 0.5909\n",
      "Epoch 9/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2540 - acc: 0.9577 - val_loss: 2.6996 - val_acc: 0.6590\n",
      "Epoch 10/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2522 - acc: 0.9578 - val_loss: 2.6275 - val_acc: 0.6750\n",
      "Epoch 11/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2501 - acc: 0.9582 - val_loss: 3.0265 - val_acc: 0.6524\n",
      "Epoch 12/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2499 - acc: 0.9587 - val_loss: 2.9269 - val_acc: 0.6470\n",
      "Epoch 13/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2474 - acc: 0.9589 - val_loss: 3.1155 - val_acc: 0.6291\n",
      "Epoch 14/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2454 - acc: 0.9596 - val_loss: 3.1129 - val_acc: 0.6782\n",
      "Epoch 15/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2443 - acc: 0.9596 - val_loss: 2.9807 - val_acc: 0.6561\n",
      "Epoch 16/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2423 - acc: 0.9598 - val_loss: 2.9776 - val_acc: 0.6381\n",
      "Epoch 17/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2426 - acc: 0.9601 - val_loss: 3.3062 - val_acc: 0.5487\n",
      "Epoch 18/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2419 - acc: 0.9602 - val_loss: 3.2440 - val_acc: 0.6092\n",
      "Epoch 19/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2398 - acc: 0.9606 - val_loss: 3.2831 - val_acc: 0.5575\n",
      "Epoch 20/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2396 - acc: 0.9609 - val_loss: 3.4728 - val_acc: 0.4927\n",
      "Epoch 21/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2379 - acc: 0.9609 - val_loss: 3.0802 - val_acc: 0.6362\n",
      "Epoch 22/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2378 - acc: 0.9614 - val_loss: 3.2671 - val_acc: 0.5611\n",
      "Epoch 23/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2359 - acc: 0.9614 - val_loss: 3.2391 - val_acc: 0.5521\n",
      "Epoch 24/200\n",
      "92000/92000 [==============================] - 45s 494us/step - loss: 0.2350 - acc: 0.9616 - val_loss: 3.4406 - val_acc: 0.5173\n",
      "Epoch 25/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2353 - acc: 0.9616 - val_loss: 3.2378 - val_acc: 0.6078\n",
      "Epoch 26/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2342 - acc: 0.9620 - val_loss: 3.1297 - val_acc: 0.6311\n",
      "Epoch 27/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2332 - acc: 0.9624 - val_loss: 3.2013 - val_acc: 0.5262\n",
      "Epoch 28/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2331 - acc: 0.9626 - val_loss: 3.2581 - val_acc: 0.5754\n",
      "Epoch 29/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2328 - acc: 0.9625 - val_loss: 3.2951 - val_acc: 0.5581\n",
      "Epoch 30/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2305 - acc: 0.9629 - val_loss: 3.4311 - val_acc: 0.4797\n",
      "Epoch 31/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2308 - acc: 0.9627 - val_loss: 3.1258 - val_acc: 0.5932\n",
      "Epoch 32/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2306 - acc: 0.9627 - val_loss: 3.1048 - val_acc: 0.5640\n",
      "Epoch 33/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2301 - acc: 0.9631 - val_loss: 3.0752 - val_acc: 0.6249\n",
      "Epoch 34/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2294 - acc: 0.9631 - val_loss: 3.2031 - val_acc: 0.5509\n",
      "Epoch 35/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2290 - acc: 0.9631 - val_loss: 3.2057 - val_acc: 0.6147\n",
      "Epoch 36/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2275 - acc: 0.9631 - val_loss: 3.2456 - val_acc: 0.6037\n",
      "Epoch 37/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2269 - acc: 0.9636 - val_loss: 3.2946 - val_acc: 0.5492\n",
      "Epoch 38/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2274 - acc: 0.9635 - val_loss: 3.0923 - val_acc: 0.6444\n",
      "Epoch 39/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2273 - acc: 0.9634 - val_loss: 3.1858 - val_acc: 0.6590\n",
      "Epoch 40/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2256 - acc: 0.9636 - val_loss: 3.2017 - val_acc: 0.6255\n",
      "Epoch 41/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2254 - acc: 0.9637 - val_loss: 3.0733 - val_acc: 0.6202\n",
      "Epoch 42/200\n",
      "92000/92000 [==============================] - 46s 495us/step - loss: 0.2249 - acc: 0.9637 - val_loss: 3.1645 - val_acc: 0.6229\n",
      "Epoch 43/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2245 - acc: 0.9639 - val_loss: 3.2234 - val_acc: 0.5236\n",
      "Epoch 44/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2237 - acc: 0.9640 - val_loss: 3.2107 - val_acc: 0.5880\n",
      "Epoch 45/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2229 - acc: 0.9640 - val_loss: 3.3895 - val_acc: 0.5465\n",
      "Epoch 46/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2226 - acc: 0.9638 - val_loss: 3.1978 - val_acc: 0.6474\n",
      "Epoch 47/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2228 - acc: 0.9639 - val_loss: 2.9446 - val_acc: 0.6220\n",
      "Epoch 48/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2216 - acc: 0.9640 - val_loss: 2.9778 - val_acc: 0.6720\n",
      "Epoch 49/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2210 - acc: 0.9643 - val_loss: 3.0198 - val_acc: 0.6500\n",
      "Epoch 50/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2209 - acc: 0.9642 - val_loss: 3.0281 - val_acc: 0.6240\n",
      "Epoch 51/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2200 - acc: 0.9641 - val_loss: 3.1199 - val_acc: 0.6549\n",
      "Epoch 52/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2197 - acc: 0.9642 - val_loss: 3.1294 - val_acc: 0.6628\n",
      "Epoch 53/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2187 - acc: 0.9643 - val_loss: 3.0889 - val_acc: 0.6791\n",
      "Epoch 54/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2188 - acc: 0.9639 - val_loss: 3.1785 - val_acc: 0.6309\n",
      "Epoch 55/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.2188 - acc: 0.9641 - val_loss: 3.0263 - val_acc: 0.6477\n",
      "Epoch 56/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2190 - acc: 0.9640 - val_loss: 3.0809 - val_acc: 0.6619\n",
      "Epoch 57/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2181 - acc: 0.9640 - val_loss: 3.0876 - val_acc: 0.6801\n",
      "Epoch 58/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2175 - acc: 0.9644 - val_loss: 2.9632 - val_acc: 0.7131\n",
      "Epoch 59/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2175 - acc: 0.9642 - val_loss: 3.1418 - val_acc: 0.6158\n",
      "Epoch 60/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2164 - acc: 0.9642 - val_loss: 2.9857 - val_acc: 0.6960\n",
      "Epoch 61/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2171 - acc: 0.9643 - val_loss: 3.2046 - val_acc: 0.6280\n",
      "Epoch 62/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2154 - acc: 0.9642 - val_loss: 3.1142 - val_acc: 0.6469\n",
      "Epoch 63/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2143 - acc: 0.9644 - val_loss: 3.0396 - val_acc: 0.6356\n",
      "Epoch 64/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2150 - acc: 0.9641 - val_loss: 3.0224 - val_acc: 0.7165\n",
      "Epoch 65/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2147 - acc: 0.9641 - val_loss: 2.9832 - val_acc: 0.7210\n",
      "Epoch 66/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2142 - acc: 0.9644 - val_loss: 3.0433 - val_acc: 0.6980\n",
      "Epoch 67/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2130 - acc: 0.9643 - val_loss: 3.1895 - val_acc: 0.6736\n",
      "Epoch 68/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2144 - acc: 0.9643 - val_loss: 3.2109 - val_acc: 0.6956\n",
      "Epoch 69/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2133 - acc: 0.9643 - val_loss: 3.3416 - val_acc: 0.6352\n",
      "Epoch 70/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2127 - acc: 0.9643 - val_loss: 3.1263 - val_acc: 0.6368\n",
      "Epoch 71/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2119 - acc: 0.9642 - val_loss: 3.2332 - val_acc: 0.6477\n",
      "Epoch 72/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2117 - acc: 0.9643 - val_loss: 3.1243 - val_acc: 0.6606\n",
      "Epoch 73/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2107 - acc: 0.9641 - val_loss: 3.1285 - val_acc: 0.6862\n",
      "Epoch 74/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2095 - acc: 0.9642 - val_loss: 3.3201 - val_acc: 0.6512\n",
      "Epoch 75/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2102 - acc: 0.9641 - val_loss: 3.2609 - val_acc: 0.6791\n",
      "Epoch 76/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2099 - acc: 0.9643 - val_loss: 3.1885 - val_acc: 0.6783\n",
      "Epoch 77/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2096 - acc: 0.9641 - val_loss: 3.2267 - val_acc: 0.6538\n",
      "Epoch 78/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2087 - acc: 0.9643 - val_loss: 3.1414 - val_acc: 0.6737\n",
      "Epoch 79/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2091 - acc: 0.9642 - val_loss: 3.0718 - val_acc: 0.6754\n",
      "Epoch 80/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2081 - acc: 0.9642 - val_loss: 3.1842 - val_acc: 0.7064\n",
      "Epoch 81/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.2076 - acc: 0.9644 - val_loss: 3.1798 - val_acc: 0.6602\n",
      "Epoch 82/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2088 - acc: 0.9643 - val_loss: 3.1318 - val_acc: 0.6627\n",
      "Epoch 83/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2066 - acc: 0.9642 - val_loss: 3.2920 - val_acc: 0.7091\n",
      "Epoch 84/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2054 - acc: 0.9643 - val_loss: 3.2366 - val_acc: 0.6658\n",
      "Epoch 85/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2047 - acc: 0.9642 - val_loss: 3.1457 - val_acc: 0.6941\n",
      "Epoch 86/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2063 - acc: 0.9644 - val_loss: 3.1344 - val_acc: 0.7175\n",
      "Epoch 87/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2048 - acc: 0.9646 - val_loss: 3.1731 - val_acc: 0.6704\n",
      "Epoch 88/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2050 - acc: 0.9640 - val_loss: 3.2250 - val_acc: 0.6939\n",
      "Epoch 89/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2035 - acc: 0.9643 - val_loss: 3.1568 - val_acc: 0.6850\n",
      "Epoch 90/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2054 - acc: 0.9639 - val_loss: 3.2241 - val_acc: 0.6993\n",
      "Epoch 91/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2037 - acc: 0.9640 - val_loss: 3.2277 - val_acc: 0.7122\n",
      "Epoch 92/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2036 - acc: 0.9642 - val_loss: 3.2115 - val_acc: 0.6306\n",
      "Epoch 93/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2038 - acc: 0.9640 - val_loss: 3.0414 - val_acc: 0.6932\n",
      "Epoch 94/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2014 - acc: 0.9643 - val_loss: 3.2243 - val_acc: 0.5797\n",
      "Epoch 95/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2023 - acc: 0.9638 - val_loss: 3.1469 - val_acc: 0.6646\n",
      "Epoch 96/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.2011 - acc: 0.9642 - val_loss: 3.2308 - val_acc: 0.6161\n",
      "Epoch 97/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.2019 - acc: 0.9643 - val_loss: 3.2246 - val_acc: 0.6382\n",
      "Epoch 98/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2022 - acc: 0.9640 - val_loss: 3.2400 - val_acc: 0.6932\n",
      "Epoch 99/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.2017 - acc: 0.9642 - val_loss: 3.2953 - val_acc: 0.6520\n",
      "Epoch 100/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1990 - acc: 0.9643 - val_loss: 3.1119 - val_acc: 0.6920\n",
      "Epoch 101/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1982 - acc: 0.9643 - val_loss: 3.2619 - val_acc: 0.6596\n",
      "Epoch 102/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1985 - acc: 0.9639 - val_loss: 3.2844 - val_acc: 0.6678\n",
      "Epoch 103/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.2010 - acc: 0.9638 - val_loss: 3.2040 - val_acc: 0.6736\n",
      "Epoch 104/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1985 - acc: 0.9643 - val_loss: 3.2963 - val_acc: 0.6358\n",
      "Epoch 105/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1992 - acc: 0.9643 - val_loss: 3.3747 - val_acc: 0.6417\n",
      "Epoch 106/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1967 - acc: 0.9643 - val_loss: 3.2446 - val_acc: 0.6680\n",
      "Epoch 107/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1976 - acc: 0.9640 - val_loss: 3.1976 - val_acc: 0.6488\n",
      "Epoch 108/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1978 - acc: 0.9643 - val_loss: 3.1735 - val_acc: 0.6342\n",
      "Epoch 109/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1953 - acc: 0.9642 - val_loss: 3.2153 - val_acc: 0.7057\n",
      "Epoch 110/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1961 - acc: 0.9642 - val_loss: 3.2361 - val_acc: 0.6755\n",
      "Epoch 111/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1953 - acc: 0.9642 - val_loss: 3.5544 - val_acc: 0.6010\n",
      "Epoch 112/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1943 - acc: 0.9641 - val_loss: 3.2036 - val_acc: 0.6915\n",
      "Epoch 113/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1945 - acc: 0.9643 - val_loss: 3.2220 - val_acc: 0.6694\n",
      "Epoch 114/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1957 - acc: 0.9640 - val_loss: 3.3371 - val_acc: 0.6367\n",
      "Epoch 115/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1961 - acc: 0.9640 - val_loss: 3.3432 - val_acc: 0.6507\n",
      "Epoch 116/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1936 - acc: 0.9645 - val_loss: 3.2797 - val_acc: 0.6668\n",
      "Epoch 117/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1947 - acc: 0.9642 - val_loss: 3.3113 - val_acc: 0.7051\n",
      "Epoch 118/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1928 - acc: 0.9644 - val_loss: 3.3561 - val_acc: 0.6773\n",
      "Epoch 119/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1936 - acc: 0.9641 - val_loss: 3.3437 - val_acc: 0.6520\n",
      "Epoch 120/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1928 - acc: 0.9644 - val_loss: 3.3406 - val_acc: 0.6455\n",
      "Epoch 121/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1915 - acc: 0.9642 - val_loss: 3.3534 - val_acc: 0.6825\n",
      "Epoch 122/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1911 - acc: 0.9646 - val_loss: 3.2894 - val_acc: 0.6568\n",
      "Epoch 123/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1907 - acc: 0.9644 - val_loss: 3.1698 - val_acc: 0.6986\n",
      "Epoch 124/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1908 - acc: 0.9642 - val_loss: 3.2777 - val_acc: 0.6857\n",
      "Epoch 125/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1903 - acc: 0.9645 - val_loss: 3.2817 - val_acc: 0.6700\n",
      "Epoch 126/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1907 - acc: 0.9645 - val_loss: 3.3009 - val_acc: 0.6707\n",
      "Epoch 127/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1891 - acc: 0.9644 - val_loss: 3.3603 - val_acc: 0.6354\n",
      "Epoch 128/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1891 - acc: 0.9646 - val_loss: 3.2434 - val_acc: 0.7055\n",
      "Epoch 129/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1889 - acc: 0.9645 - val_loss: 3.4136 - val_acc: 0.6724\n",
      "Epoch 130/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1891 - acc: 0.9645 - val_loss: 3.2839 - val_acc: 0.6630\n",
      "Epoch 131/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1872 - acc: 0.9645 - val_loss: 3.3061 - val_acc: 0.6449\n",
      "Epoch 132/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1880 - acc: 0.9646 - val_loss: 3.2563 - val_acc: 0.6710\n",
      "Epoch 133/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1857 - acc: 0.9647 - val_loss: 3.1337 - val_acc: 0.6976\n",
      "Epoch 134/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1879 - acc: 0.9645 - val_loss: 3.2775 - val_acc: 0.6764\n",
      "Epoch 135/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1878 - acc: 0.9644 - val_loss: 3.2912 - val_acc: 0.6573\n",
      "Epoch 136/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1862 - acc: 0.9647 - val_loss: 3.3811 - val_acc: 0.6248\n",
      "Epoch 137/200\n",
      "92000/92000 [==============================] - 46s 500us/step - loss: 0.1858 - acc: 0.9646 - val_loss: 3.2244 - val_acc: 0.6702\n",
      "Epoch 138/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1853 - acc: 0.9648 - val_loss: 3.2711 - val_acc: 0.6522\n",
      "Epoch 139/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1855 - acc: 0.9649 - val_loss: 3.3450 - val_acc: 0.6324\n",
      "Epoch 140/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1839 - acc: 0.9645 - val_loss: 3.2478 - val_acc: 0.6502\n",
      "Epoch 141/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1839 - acc: 0.9644 - val_loss: 3.3244 - val_acc: 0.6592\n",
      "Epoch 142/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1843 - acc: 0.9644 - val_loss: 3.2435 - val_acc: 0.6556\n",
      "Epoch 143/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1832 - acc: 0.9646 - val_loss: 3.2847 - val_acc: 0.6701\n",
      "Epoch 144/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1828 - acc: 0.9648 - val_loss: 3.2669 - val_acc: 0.6724\n",
      "Epoch 145/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1825 - acc: 0.9648 - val_loss: 3.3249 - val_acc: 0.6119\n",
      "Epoch 146/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1834 - acc: 0.9644 - val_loss: 3.3723 - val_acc: 0.6287\n",
      "Epoch 147/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1842 - acc: 0.9646 - val_loss: 3.3784 - val_acc: 0.6231\n",
      "Epoch 148/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1808 - acc: 0.9647 - val_loss: 3.1941 - val_acc: 0.6944\n",
      "Epoch 149/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1812 - acc: 0.9648 - val_loss: 3.4641 - val_acc: 0.5655\n",
      "Epoch 150/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1836 - acc: 0.9646 - val_loss: 3.2406 - val_acc: 0.6653\n",
      "Epoch 151/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1811 - acc: 0.9647 - val_loss: 3.3031 - val_acc: 0.6522\n",
      "Epoch 152/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1804 - acc: 0.9648 - val_loss: 3.4466 - val_acc: 0.5953\n",
      "Epoch 153/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.1799 - acc: 0.9648 - val_loss: 3.4116 - val_acc: 0.6408\n",
      "Epoch 154/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1793 - acc: 0.9648 - val_loss: 3.4653 - val_acc: 0.5967\n",
      "Epoch 155/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1795 - acc: 0.9649 - val_loss: 3.3761 - val_acc: 0.6188\n",
      "Epoch 156/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1797 - acc: 0.9648 - val_loss: 3.3970 - val_acc: 0.6556\n",
      "Epoch 157/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1774 - acc: 0.9652 - val_loss: 3.4854 - val_acc: 0.6313\n",
      "Epoch 158/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1784 - acc: 0.9649 - val_loss: 3.4455 - val_acc: 0.5966\n",
      "Epoch 159/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1786 - acc: 0.9647 - val_loss: 3.5021 - val_acc: 0.5702\n",
      "Epoch 160/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1778 - acc: 0.9652 - val_loss: 3.5976 - val_acc: 0.5830\n",
      "Epoch 161/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1770 - acc: 0.9650 - val_loss: 3.3366 - val_acc: 0.6323\n",
      "Epoch 162/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1767 - acc: 0.9652 - val_loss: 3.4560 - val_acc: 0.6170\n",
      "Epoch 163/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1766 - acc: 0.9651 - val_loss: 3.2725 - val_acc: 0.6593\n",
      "Epoch 164/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.1769 - acc: 0.9651 - val_loss: 3.3457 - val_acc: 0.6643\n",
      "Epoch 165/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1767 - acc: 0.9649 - val_loss: 3.3626 - val_acc: 0.6818\n",
      "Epoch 166/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1767 - acc: 0.9650 - val_loss: 3.2679 - val_acc: 0.6953\n",
      "Epoch 167/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1764 - acc: 0.9649 - val_loss: 3.4912 - val_acc: 0.6237\n",
      "Epoch 168/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.1745 - acc: 0.9651 - val_loss: 3.3313 - val_acc: 0.6507\n",
      "Epoch 169/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1751 - acc: 0.9652 - val_loss: 3.4150 - val_acc: 0.6170\n",
      "Epoch 170/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1744 - acc: 0.9652 - val_loss: 3.4178 - val_acc: 0.6228\n",
      "Epoch 171/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1741 - acc: 0.9653 - val_loss: 3.4372 - val_acc: 0.6316\n",
      "Epoch 172/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1737 - acc: 0.9651 - val_loss: 3.3864 - val_acc: 0.6542\n",
      "Epoch 173/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1728 - acc: 0.9652 - val_loss: 3.3384 - val_acc: 0.6533\n",
      "Epoch 174/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1730 - acc: 0.9653 - val_loss: 3.4536 - val_acc: 0.6632\n",
      "Epoch 175/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1719 - acc: 0.9651 - val_loss: 3.3456 - val_acc: 0.6610\n",
      "Epoch 176/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1728 - acc: 0.9651 - val_loss: 3.5056 - val_acc: 0.6454\n",
      "Epoch 177/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1730 - acc: 0.9651 - val_loss: 3.3980 - val_acc: 0.6629\n",
      "Epoch 178/200\n",
      "92000/92000 [==============================] - 46s 499us/step - loss: 0.1727 - acc: 0.9650 - val_loss: 3.2444 - val_acc: 0.6563\n",
      "Epoch 179/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1724 - acc: 0.9651 - val_loss: 3.5285 - val_acc: 0.6295\n",
      "Epoch 180/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1703 - acc: 0.9655 - val_loss: 3.4488 - val_acc: 0.6058\n",
      "Epoch 181/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1717 - acc: 0.9651 - val_loss: 3.4652 - val_acc: 0.6233\n",
      "Epoch 182/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1704 - acc: 0.9653 - val_loss: 3.6052 - val_acc: 0.6276\n",
      "Epoch 183/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1703 - acc: 0.9653 - val_loss: 3.4256 - val_acc: 0.6569\n",
      "Epoch 184/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1687 - acc: 0.9655 - val_loss: 3.4451 - val_acc: 0.6400\n",
      "Epoch 185/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1697 - acc: 0.9652 - val_loss: 3.3989 - val_acc: 0.6378\n",
      "Epoch 186/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1705 - acc: 0.9654 - val_loss: 3.4491 - val_acc: 0.6045\n",
      "Epoch 187/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1716 - acc: 0.9653 - val_loss: 3.5464 - val_acc: 0.6132\n",
      "Epoch 188/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1692 - acc: 0.9655 - val_loss: 3.5743 - val_acc: 0.6326\n",
      "Epoch 189/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1688 - acc: 0.9654 - val_loss: 3.4619 - val_acc: 0.6180\n",
      "Epoch 190/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1704 - acc: 0.9658 - val_loss: 3.4759 - val_acc: 0.6738\n",
      "Epoch 191/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1679 - acc: 0.9658 - val_loss: 3.4547 - val_acc: 0.6195\n",
      "Epoch 192/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1679 - acc: 0.9656 - val_loss: 3.4581 - val_acc: 0.6524\n",
      "Epoch 193/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1695 - acc: 0.9654 - val_loss: 3.3684 - val_acc: 0.6814\n",
      "Epoch 194/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1683 - acc: 0.9656 - val_loss: 3.5940 - val_acc: 0.5864\n",
      "Epoch 195/200\n",
      "92000/92000 [==============================] - 46s 496us/step - loss: 0.1697 - acc: 0.9652 - val_loss: 3.3695 - val_acc: 0.6481\n",
      "Epoch 196/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1665 - acc: 0.9656 - val_loss: 3.3717 - val_acc: 0.6827\n",
      "Epoch 197/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1657 - acc: 0.9658 - val_loss: 3.5073 - val_acc: 0.6601\n",
      "Epoch 198/200\n",
      "92000/92000 [==============================] - 46s 498us/step - loss: 0.1653 - acc: 0.9657 - val_loss: 3.5240 - val_acc: 0.6102\n",
      "Epoch 199/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1655 - acc: 0.9657 - val_loss: 3.5240 - val_acc: 0.6299\n",
      "Epoch 200/200\n",
      "92000/92000 [==============================] - 46s 497us/step - loss: 0.1658 - acc: 0.9658 - val_loss: 3.5595 - val_acc: 0.6195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a8cca74c18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#g = corpus_training_data_generator('gutenberg',TIME_SLICE_SIZE, BATCH_SIZE, shift=False)\n",
    "model.fit(X, Y, validation_split=0.2, callbacks=[tensor_board], batch_size=BATCH_SIZE, epochs=EPOCHS, sample_weight=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tim Smith Works for google in california'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentence = \"Tim Smith works for Google in California\".lower()\n",
    "test_sentence = pad([list(original_sentence)], len(original_sentence))\n",
    "test_sentence = encode_each_sentence(test_sentence, input_feature_to_int_map)\n",
    "\n",
    "mapped_sentence = np.asarray(test_sentence)\n",
    "predicted_result = model.predict_classes(mapped_sentence)[0]\n",
    "print(predicted_result)\n",
    "predicted_result = list(zip(pad([list(original_sentence)], len(original_sentence))[0], predicted_result.tolist()))\n",
    "\n",
    "def true_case(letter, label):\n",
    "    if letter == 0:\n",
    "        return ''\n",
    "    if(label == 0):\n",
    "        return letter.lower()\n",
    "    if (label == 1):\n",
    "        return letter.upper()\n",
    "    return letter\n",
    "\n",
    "predicted_result = [true_case(letter, label) for letter, label in predicted_result]\n",
    "''.join(predicted_result) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"big_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.]\n",
      "[56.]\n",
      "[27.]\n",
      "[36.]\n",
      "[54.]\n",
      "[20.]\n",
      "[2.]\n",
      "[50.]\n",
      "[2.]\n",
      "[44.]\n",
      "[23.]\n",
      "[42.]\n",
      "[57.]\n",
      "[61.]\n",
      "[20.]\n",
      "[33.]\n",
      "[2.]\n",
      "[50.]\n",
      "[2.]\n",
      "[20.]\n",
      "[28.]\n",
      "[28.]\n",
      "[42.]\n",
      "[2.]\n",
      "[64.]\n",
      "[30.]\n",
      "[56.]\n",
      "[45.]\n",
      "[49.]\n",
      "[30.]\n",
      "[36.]\n",
      "[34.]\n",
      "[20.]\n",
      "[3.]\n",
      "[2.]\n",
      "[23.]\n",
      "[16.]\n",
      "[29.]\n",
      "[45.]\n",
      "[60.]\n",
      "[30.]\n",
      "[28.]\n",
      "[20.]\n",
      "[3.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x in X[0]:\n",
    "    print(x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
